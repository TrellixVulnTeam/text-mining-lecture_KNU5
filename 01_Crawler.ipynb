{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Collection: Building an Instagram Crawler"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this walkthrough we will see how we can crawl data from Instagram using the Selenium WebDriver."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 1: Set up the WebDriver\n",
    "\n",
    "The first thing we need is a web driver. A web driver is basically a browser that can be controlled programatically."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "# Set up a function to start the webdriver\n",
    "from selenium import webdriver\n",
    "def start_webdriver():\n",
    "    chromedriver_path = 'helpers/chromedriver'\n",
    "    driver = webdriver.Chrome(chromedriver_path)\n",
    "    return driver"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "# Execute the webdriver\n",
    "driver = start_webdriver()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 2: Navigate to Instagram and Login\n",
    "In order to log in we need to mimic the same user flow that we would use to log on manually. To do so, we use the Google Developer Tools to find the selectors / clases of the respective buttons and form fields."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "driver.get(\"https://www.instagram.com/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "# Import packages that we need\n",
    "from random import randint\n",
    "import time\n",
    "import json"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "# Define Login Helper Function\n",
    "def login():\n",
    "  try:\n",
    "    # the login_info.json file contains the login information for five different Instagram accounts. We randomly pick one of the five accounts and save the username and password in respective variables.\n",
    "    with open('helpers/login_info.json') as f:\n",
    "        login_info = json.load(f)\n",
    "    random_number = randint(0, len(login_info['accounts']) - 1)\n",
    "    user = login_info['accounts'][random_number]['username']\n",
    "    pw = login_info['accounts'][random_number]['pw']\n",
    "    # We go to the Instagram home page\n",
    "    driver.get(\"https://www.instagram.com/\")\n",
    "    # We wait five seconds in order to make sure the page has fully loaded\n",
    "    time.sleep(5)\n",
    "    # We programmatically close the cookie notice\n",
    "    try:\n",
    "      driver.find_element_by_css_selector('.aOOlW').click()\n",
    "    except:\n",
    "      pass\n",
    "    time.sleep(3)\n",
    "    # We find the username and password fields and make sure they are empty\n",
    "    username = driver.find_element_by_css_selector(\"input[name='username']\")\n",
    "    password = driver.find_element_by_css_selector(\"input[name='password']\")\n",
    "    username.clear()\n",
    "    password.clear()\n",
    "    # We enter the login credentials into the form and then programmatically click the login button.\n",
    "    username.send_keys(user)\n",
    "    password.send_keys(pw)\n",
    "    time.sleep(randint(3,5))\n",
    "    login = driver.find_element_by_css_selector(\"button[type='submit']\").click()\n",
    "  except:\n",
    "    print(\"Could not log in / already logged in\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "# Define Logout Helper Function\n",
    "def logout():\n",
    "  try:\n",
    "    # We go to the Instagram home page\n",
    "    driver.get(\"https://www.instagram.com/\")\n",
    "    time.sleep(5)\n",
    "    # We open the dropdown menu\n",
    "    menu = driver.find_element_by_class_name(\"_47KiJ\")\n",
    "    menu.find_element_by_class_name(\"_2dbep\").click()\n",
    "    time.sleep(3)\n",
    "    # We click on the logout button\n",
    "    menu.find_element_by_css_selector('.-qQT3:last-child').click()\n",
    "    time.sleep(1)\n",
    "    # We delete all cookies so that Instagram does not \n",
    "    # modify the login process when we want to log in again\n",
    "    driver.delete_all_cookies()\n",
    "  except:\n",
    "    print(\"Could not log out / already logged out\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "# Execute Login Helper Function\n",
    "login()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "# Execute Logout Helper Function\n",
    "logout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 3: Decide which Accounts / Companies to Crawl and Create a Folder for Each"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "# For this tutorial we take three big FMCG brands as an example\n",
    "companies = ['nestle', 'unilever', 'proctergamble']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "# Import the os package\n",
    "import os\n",
    "# Define function that checks if a folder for the company exists within the example data folder. If not, it creates one.\n",
    "def create_company_folder(company):\n",
    "  path = os.getcwd() + \"/example_data/\" + company + \"/\"\n",
    "  if not os.path.exists(path):\n",
    "      os.makedirs(path)\n",
    "  return path\n",
    "# Execute the function\n",
    "for company in companies:\n",
    "  create_company_folder(company)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "# Check out one of the accounts to plan the next steps\n",
    "driver.get(\"https://www.instagram.com/\" + companies[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 4: Get Post URLs\n",
    "In order to crawl the content of all Instagram posts of the selected accounts, we need their respective URLs. We get these by programatically scrolling to the bottom of the page and saving the URLs (href-attribute of the pictures) after every scroll action.\n",
    "\n",
    "For this tutorial we limit the number of scrolled posts to 100 per company."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "#check if hrefs.txt file already exists in the folder\n",
    "from pathlib import Path\n",
    "def find_hrefs(company):\n",
    "    # Define the file in which all URLs will be saved in the end\n",
    "    href_file = os.getcwd() + \"/example_data/\" + company + \"/hrefs.txt\"\n",
    "    # Check if the file already exists\n",
    "    href_file_exists = Path(href_file).is_file()\n",
    "    # if the file does not exist, start scraping the URLs\n",
    "    if not href_file_exists:\n",
    "        print(\"no URLs found for \" + company + \", starting to scrape them\")\n",
    "        time.sleep(randint(3, 5))\n",
    "        # Go to the company's Instagram account\n",
    "        driver.get(\"https://www.instagram.com/\" + company)\n",
    "        # Scrape all URLs, then scroll down and scrape the newly loaded posts until the end of the page is reached\n",
    "        # For this tutorial we also limit the number of scrolled posts to 100 per company\n",
    "        hrefs = []\n",
    "        scrolldown = 0\n",
    "        match = False\n",
    "        while (match == False and len(hrefs) < 100):\n",
    "            last_count = scrolldown\n",
    "            # Find all links that are currently displayed on the page\n",
    "            links = driver.find_elements_by_tag_name('a')\n",
    "            time.sleep(randint(3, 4))\n",
    "            for link in links:\n",
    "                try:\n",
    "                    href = link.get_attribute('href')\n",
    "                    # only take links that include \"/p/\", indicating that it is a post link\n",
    "                    if '/p/' in href:\n",
    "                        # only add the post to the list of URLs if it is not in the list yet (prevent duplicates)\n",
    "                        if href not in hrefs:\n",
    "                            if len(hrefs) < 100:\n",
    "                                hrefs.append(href)\n",
    "                except:\n",
    "                    pass\n",
    "            # scroll to the bottom of the page to load new posts\n",
    "            scrolldown = driver.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);var scrolldown=document.body.scrollHeight;return scrolldown;\")\n",
    "            # stop the process when the end of the page is reached or we have collected at least 100 URLs\n",
    "            if last_count == scrolldown or len(hrefs) > 99:\n",
    "                match = True\n",
    "        print(\"saving \" + str(len(hrefs)) + \" URLs to file for \" + company)\n",
    "        with open(href_file, 'w') as file:\n",
    "            file.write(str(hrefs))\n",
    "    else:\n",
    "        print(\"URLs file discovered\")\n",
    "        file = open(href_file, 'r')\n",
    "        hrefs = eval(file.read())\n",
    "    return hrefs, href_file_exists"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "# Execute the Scraping Function, iterating over the companies\n",
    "for company in companies:\n",
    "  find_hrefs(company)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "no URLs found for nestle, starting to scrape them\n",
      "saving 100 to file for nestle\n",
      "no URLs found for unilever, starting to scrape them\n",
      "saving 100 to file for unilever\n",
      "no URLs found for proctergamble, starting to scrape them\n",
      "saving 100 to file for proctergamble\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 5: Iterate of the URLs and save post data as raw .json-files\n",
    "Fortunately, we can leverage the Instagram GraphQL API to retrieve posts as structured data (otherwise we would have to go through all the different HTML elements that contain the information we need and scrape it from there). To retrieve a post as structured data we just add \"?__a=1\" to the URL. This yields a .json (Javascript Object Notation) file that we can save to our harddrive.\n",
    "\n",
    "There is a caveat: images are only saved as URLs. However, Instagram periodically changes image URLs. Therefore, we need to save the images on our own harddrive or some other service (see next steps)."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "44cf03960ceb7c4dae63840fdb07d1a636e2d44365fb83c3782c6fb7da7fa481"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}